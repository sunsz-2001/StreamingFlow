#!/usr/bin/env python3
"""
ODEæ¨¡å—éªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯ODEæ¨¡å—åœ¨æ–°ç¯å¢ƒä¸­çš„æ­£ç¡®æ€§å’Œå…¼å®¹æ€§ã€‚
è¿è¡Œæ­¤è„šæœ¬å¯ä»¥å¿«é€Ÿæ£€æŸ¥ODEæ¨¡å—æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œã€‚
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
import time
import warnings
from typing import Tuple, Dict, Any

# æŠ‘åˆ¶è­¦å‘Šä»¥è·å¾—æ›´æ¸…æ™°çš„è¾“å‡º
warnings.filterwarnings('ignore')

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–é¡¹...")

    required_packages = {
        'torch': torch.__version__,
        'numpy': np.__version__,
    }

    print("âœ… ä¾èµ–é¡¹æ£€æŸ¥:")
    for package, version in required_packages.items():
        print(f"   {package}: {version}")

    # æ£€æŸ¥PyTorchåŠŸèƒ½
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}")
        print(f"   å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")

    return True


def test_ode_import():
    """æµ‹è¯•ODEæ¨¡å—å¯¼å…¥"""
    print("\nğŸ“¦ æµ‹è¯•ODEæ¨¡å—å¯¼å…¥...")

    try:
        # å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

        from ode_modules import NNFOwithBayesianJumps, FuturePredictionODE
        from ode_modules.configs.minimal_ode_config import create_minimal_ode_config
        from ode_modules.configs.config_validator import validate_ode_config

        print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•é…ç½®åˆ›å»º
        cfg = create_minimal_ode_config()
        is_valid, missing, warnings_list = validate_ode_config(cfg)

        if is_valid:
            print("âœ… é…ç½®éªŒè¯é€šè¿‡")
        else:
            print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {missing}")
            return False

        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        model1 = NNFOwithBayesianJumps(input_size=64, hidden_size=64, cfg=cfg)
        model2 = FuturePredictionODE(in_channels=64, latent_dim=64, cfg=cfg)

        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        return True, (NNFOwithBayesianJumps, FuturePredictionODE, create_minimal_ode_config)

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("   è¯·æ£€æŸ¥ODEæ¨¡å—è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return False, None

    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return False, None


def test_forward_pass(NNFOwithBayesianJumps, create_minimal_ode_config):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nğŸ§  æµ‹è¯•å‰å‘ä¼ æ’­...")

    try:
        # åˆ›å»ºæ¨¡å‹
        cfg = create_minimal_ode_config()
        model = NNFOwithBayesianJumps(input_size=64, hidden_size=64, cfg=cfg)
        model.eval()

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        batch_size = 2
        current_input = torch.randn(batch_size, 1, 64, 32, 32)
        observations = torch.randn(batch_size, 3, 64, 32, 32)
        times = torch.tensor([0.0, 0.5, 1.0])
        target_times = torch.tensor([1.5, 2.0])

        print(f"   è¾“å…¥å½¢çŠ¶: {current_input.shape}")
        print(f"   è§‚æµ‹å½¢çŠ¶: {observations.shape}")

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            start_time = time.time()
            final_state, loss, predictions = model(
                times=times,
                input=current_input,
                obs=observations,
                delta_t=0.1,
                T=target_times
            )
            end_time = time.time()

        # éªŒè¯è¾“å‡º
        expected_pred_shape = (batch_size, len(target_times), 64, 32, 32)
        if predictions.shape == expected_pred_shape:
            print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {predictions.shape}")
        else:
            print(f"âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_pred_shape}, å¾—åˆ° {predictions.shape}")
            return False

        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isfinite(predictions).all():
            print("âœ… è¾“å‡ºæ•°å€¼ç¨³å®š")
        else:
            print("âŒ è¾“å‡ºåŒ…å«NaNæˆ–Inf")
            return False

        print(f"âœ… å‰å‘ä¼ æ’­è€—æ—¶: {(end_time - start_time)*1000:.2f}ms")

        return True

    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gradient_flow(NNFOwithBayesianJumps, create_minimal_ode_config):
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("\nğŸ”„ æµ‹è¯•æ¢¯åº¦æµ...")

    try:
        # åˆ›å»ºæ¨¡å‹
        cfg = create_minimal_ode_config()
        model = NNFOwithBayesianJumps(input_size=32, hidden_size=32, cfg=cfg)  # å°æ¨¡å‹åŠ å¿«æµ‹è¯•
        model.train()

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

        # å‡†å¤‡æ•°æ®
        batch_size = 1
        current_input = torch.randn(batch_size, 1, 32, 16, 16, requires_grad=True)
        observations = torch.randn(batch_size, 2, 32, 16, 16)
        times = torch.tensor([0.0, 0.5])
        target_times = torch.tensor([1.0])

        # å‰å‘ä¼ æ’­
        final_state, ode_loss, predictions = model(
            times=times,
            input=current_input,
            obs=observations,
            delta_t=0.1,
            T=target_times
        )

        # è®¡ç®—æŸå¤±
        target = torch.randn_like(predictions)
        loss = nn.MSELoss()(predictions, target) + ode_loss

        print(f"   æ€»æŸå¤±: {loss.item():.6f}")

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        total_grad_norm = 0
        param_count = 0
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                total_grad_norm += grad_norm
                param_count += 1
            else:
                print(f"âš ï¸ å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦")

        avg_grad_norm = total_grad_norm / param_count if param_count > 0 else 0

        print(f"   å‚æ•°æ•°é‡: {param_count}")
        print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")

        if avg_grad_norm > 0:
            print("âœ… æ¢¯åº¦æµæ­£å¸¸")
        else:
            print("âŒ æ¢¯åº¦æµå¼‚å¸¸")
            return False

        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.step()
        print("âœ… ä¼˜åŒ–å™¨æ›´æ–°æˆåŠŸ")

        return True

    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_memory_usage(NNFOwithBayesianJumps, create_minimal_ode_config):
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    print("\nğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨...")

    try:
        if not torch.cuda.is_available():
            print("   è·³è¿‡GPUå†…å­˜æµ‹è¯• (CUDAä¸å¯ç”¨)")
            return True

        # æ¸…ç©ºGPUç¼“å­˜
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated()

        # åˆ›å»ºæ¨¡å‹
        cfg = create_minimal_ode_config()
        model = NNFOwithBayesianJumps(input_size=64, hidden_size=64, cfg=cfg).cuda()

        model_memory = torch.cuda.memory_allocated() - initial_memory
        print(f"   æ¨¡å‹å†…å­˜: {model_memory / 1e6:.2f} MB")

        # æµ‹è¯•æ‰¹æ¬¡å¤§å°
        batch_sizes = [1, 2, 4]
        for batch_size in batch_sizes:
            torch.cuda.empty_cache()
            start_memory = torch.cuda.memory_allocated()

            # å‡†å¤‡æ•°æ®
            current_input = torch.randn(batch_size, 1, 64, 32, 32).cuda()
            observations = torch.randn(batch_size, 3, 64, 32, 32).cuda()
            times = torch.tensor([0.0, 0.5, 1.0])
            target_times = torch.tensor([1.5, 2.0])

            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                final_state, loss, predictions = model(
                    times=times,
                    input=current_input,
                    obs=observations,
                    delta_t=0.1,
                    T=target_times
                )

            peak_memory = torch.cuda.max_memory_allocated() - start_memory
            print(f"   æ‰¹æ¬¡å¤§å° {batch_size}: {peak_memory / 1e6:.2f} MB")

            # é‡ç½®å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()

        print("âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_different_configs(NNFOwithBayesianJumps, create_minimal_ode_config):
    """æµ‹è¯•ä¸åŒé…ç½®"""
    print("\nâš™ï¸ æµ‹è¯•ä¸åŒé…ç½®...")

    # å¯¼å…¥è‡ªå®šä¹‰é…ç½®å‡½æ•°
    try:
        from ode_modules.configs.minimal_ode_config import create_custom_ode_config
    except ImportError:
        print("   è·³è¿‡é…ç½®æµ‹è¯• (æ— æ³•å¯¼å…¥create_custom_ode_config)")
        return True

    configs = [
        ("æœ€å°é…ç½®", create_minimal_ode_config()),
        ("å°æ¨¡å‹", create_custom_ode_config(out_channels=32, latent_dim=32)),
        ("æ ‡å‡†æ¨¡å‹", create_custom_ode_config(out_channels=64, latent_dim=64)),
        ("ä¸­ç‚¹æ±‚è§£å™¨", create_custom_ode_config(out_channels=32, latent_dim=32, solver="midpoint")),
    ]

    for config_name, cfg in configs:
        try:
            print(f"   æµ‹è¯• {config_name}...")

            model = NNFOwithBayesianJumps(
                input_size=cfg.MODEL.ENCODER.OUT_CHANNELS,
                hidden_size=cfg.MODEL.DISTRIBUTION.LATENT_DIM,
                cfg=cfg
            )

            # å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•
            batch_size = 1
            channels = cfg.MODEL.ENCODER.OUT_CHANNELS
            current_input = torch.randn(batch_size, 1, channels, 16, 16)
            observations = torch.randn(batch_size, 2, channels, 16, 16)
            times = torch.tensor([0.0, 0.5])
            target_times = torch.tensor([1.0])

            with torch.no_grad():
                final_state, loss, predictions = model(
                    times=times,
                    input=current_input,
                    obs=observations,
                    delta_t=0.1,
                    T=target_times
                )

            print(f"     âœ… {config_name} å·¥ä½œæ­£å¸¸")

        except Exception as e:
            print(f"     âŒ {config_name} å¤±è´¥: {e}")
            return False

    print("âœ… é…ç½®æµ‹è¯•å®Œæˆ")
    return True


def run_comprehensive_validation():
    """è¿è¡Œç»¼åˆéªŒè¯"""
    print("ğŸ¯ ODEæ¨¡å—ç»¼åˆéªŒè¯")
    print("=" * 50)

    results = {}

    # 1. æ£€æŸ¥ä¾èµ–
    results['dependencies'] = check_dependencies()

    # 2. æµ‹è¯•å¯¼å…¥
    import_result, modules = test_ode_import()
    results['import'] = import_result

    if not import_result:
        print("\nâŒ å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return results

    NNFOwithBayesianJumps, FuturePredictionODE, create_minimal_ode_config = modules

    # 3. æµ‹è¯•å‰å‘ä¼ æ’­
    results['forward_pass'] = test_forward_pass(NNFOwithBayesianJumps, create_minimal_ode_config)

    # 4. æµ‹è¯•æ¢¯åº¦æµ
    results['gradient_flow'] = test_gradient_flow(NNFOwithBayesianJumps, create_minimal_ode_config)

    # 5. æµ‹è¯•å†…å­˜ä½¿ç”¨
    results['memory_usage'] = test_memory_usage(NNFOwithBayesianJumps, create_minimal_ode_config)

    # 6. æµ‹è¯•ä¸åŒé…ç½®
    results['different_configs'] = test_different_configs(NNFOwithBayesianJumps, create_minimal_ode_config)

    return results


def print_summary(results: Dict[str, bool]):
    """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
    print("\n" + "=" * 50)
    print("ğŸ“Š éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 50)

    total_tests = len(results)
    passed_tests = sum(results.values())

    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:20} {status}")

    print("-" * 50)
    print(f"æ€»è®¡: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")

    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡! ODEæ¨¡å—å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        print("âœ… æ‚¨å¯ä»¥å®‰å…¨åœ°åœ¨é¡¹ç›®ä¸­é›†æˆODEæ¨¡å—ã€‚")
    else:
        print(f"\nâš ï¸ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ã€‚")
        print("âŒ è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•å¹¶è§£å†³é—®é¢˜åå†ä½¿ç”¨ã€‚")

    print("\nğŸ“š ä½¿ç”¨æŒ‡å—:")
    print("   - æŸ¥çœ‹ INTEGRATION_GUIDE.md äº†è§£é›†æˆæ–¹æ³•")
    print("   - æŸ¥çœ‹ examples/standalone_example.py äº†è§£ä½¿ç”¨ç¤ºä¾‹")
    print("   - æŸ¥çœ‹ configs/README.md äº†è§£é…ç½®é€‰é¡¹")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ ODEæ¨¡å—éªŒè¯è„šæœ¬")
    print("ç‰ˆæœ¬: 1.0")
    print("ç›®çš„: éªŒè¯ODEæ¨¡å—åœ¨æ–°ç¯å¢ƒä¸­çš„å…¼å®¹æ€§")
    print("")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    try:
        results = run_comprehensive_validation()
        print_summary(results)

        # è¿”å›é€€å‡ºç 
        if all(results.values()):
            return 0  # æˆåŠŸ
        else:
            return 1  # å¤±è´¥

    except KeyboardInterrupt:
        print("\nâ¹ï¸ éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        return 2

    except Exception as e:
        print(f"\nğŸ’¥ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 3


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)