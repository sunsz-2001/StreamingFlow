# ODEæ¨¡å—é›†æˆæŒ‡å—

## ğŸ¯ åœ¨å…¶ä»–ç³»ç»Ÿä¸­ä½¿ç”¨ODEæ¨¡å—

è¿™ä¸ªæŒ‡å—å°†æ•™ä½ å¦‚ä½•åœ¨ä»»ä½•PyTorché¡¹ç›®ä¸­é›†æˆå’Œä½¿ç”¨ODEæ¨¡å—ã€‚

## ğŸ“¦ 1. å®‰è£…å’Œå¯¼å…¥

### æ–¹æ³•1: ç›´æ¥å¤åˆ¶æ¨¡å— (æ¨è)

```bash
# å°†æ•´ä¸ªode_modulesæ–‡ä»¶å¤¹å¤åˆ¶åˆ°ä½ çš„é¡¹ç›®ä¸­
cp -r /path/to/StreamingFlow/streamingflow/ode_modules /your/project/path/

# é¡¹ç›®ç»“æ„åº”è¯¥æ˜¯:
your_project/
â”œâ”€â”€ ode_modules/          # å¤åˆ¶çš„ODEæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cores/
â”‚   â”œâ”€â”€ cells/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ configs/
â”œâ”€â”€ your_model.py         # ä½ çš„æ¨¡å‹ä»£ç 
â””â”€â”€ main.py              # ä¸»ç¨‹åº
```

### æ–¹æ³•2: PythonåŒ…å®‰è£…

```python
# åœ¨ä½ çš„é¡¹ç›®ä¸­å¯¼å…¥
import sys
sys.path.append('/path/to/StreamingFlow')  # æ·»åŠ StreamingFlowè·¯å¾„

from streamingflow.ode_modules import NNFOwithBayesianJumps, FuturePredictionODE
```

## ğŸš€ 2. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### 2.1 æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼

```python
import torch
from ode_modules import NNFOwithBayesianJumps
from ode_modules.configs.minimal_ode_config import create_minimal_ode_config

# åˆ›å»ºé…ç½®
cfg = create_minimal_ode_config()

# åˆ›å»ºæ¨¡å‹
ode_model = NNFOwithBayesianJumps(
    input_size=64,    # è¾“å…¥ç‰¹å¾é€šé“æ•°
    hidden_size=64,   # éšè—å±‚ç»´åº¦
    cfg=cfg
)

# å‡†å¤‡è¾“å…¥æ•°æ®
batch_size = 2
seq_len = 3
channels = 64
height, width = 32, 32

# è¾“å…¥æ•°æ®æ ¼å¼: [B, 1, C, H, W]
current_input = torch.randn(batch_size, 1, channels, height, width)

# è§‚æµ‹åºåˆ—: [B, T, C, H, W]
observations = torch.randn(batch_size, seq_len, channels, height, width)

# æ—¶é—´æˆ³
times = torch.tensor([0.0, 0.5, 1.0])  # è§‚æµ‹æ—¶é—´ç‚¹
target_times = torch.tensor([1.5, 2.0, 2.5])  # é¢„æµ‹æ—¶é—´ç‚¹

# å‰å‘ä¼ æ’­
final_state, loss, predictions = ode_model(
    times=times,
    input=current_input,
    obs=observations,
    delta_t=0.1,
    T=target_times
)

print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")  # [B, T_future, C, H, W]
```

### 2.2 é›†æˆåˆ°è‡ªå®šä¹‰æ¨¡å‹ä¸­

```python
import torch
import torch.nn as nn
from ode_modules import NNFOwithBayesianJumps, FuturePredictionODE
from ode_modules.configs.minimal_ode_config import create_custom_ode_config

class MyVideoModelWithODE(nn.Module):
    """
    ç¤ºä¾‹: å°†ODEæ¨¡å—é›†æˆåˆ°è§†é¢‘é¢„æµ‹æ¨¡å‹ä¸­
    """

    def __init__(self, input_channels=3, feature_channels=64, num_future_frames=4):
        super().__init__()

        # ç‰¹å¾æå–å™¨ (ä¾‹å¦‚CNNç¼–ç å™¨)
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, feature_channels, 3, padding=1),
            nn.ReLU()
        )

        # åˆ›å»ºODEé…ç½®
        self.cfg = create_custom_ode_config(
            out_channels=feature_channels,
            latent_dim=feature_channels,
            solver="euler",
            delta_t=0.05
        )

        # è´å¶æ–¯ODEæ¨¡å—ç”¨äºæ—¶åºå»ºæ¨¡
        self.ode_predictor = NNFOwithBayesianJumps(
            input_size=feature_channels,
            hidden_size=feature_channels,
            cfg=self.cfg
        )

        # è¾“å‡ºè§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Conv2d(feature_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, input_channels, 3, padding=1),
            nn.Sigmoid()
        )

        self.num_future_frames = num_future_frames

    def forward(self, video_sequence, target_times=None):
        """
        Args:
            video_sequence: [B, T, C, H, W] è¾“å…¥è§†é¢‘åºåˆ—
            target_times: [T_future] ç›®æ ‡é¢„æµ‹æ—¶é—´ç‚¹

        Returns:
            predicted_frames: [B, T_future, C, H, W] é¢„æµ‹çš„æœªæ¥å¸§
        """
        batch_size, seq_len, channels, height, width = video_sequence.shape

        # 1. ç‰¹å¾æå–
        # å°†æ—¶åºç»´åº¦å±•å¹³è¿›è¡Œç‰¹å¾æå–
        flat_frames = video_sequence.view(-1, channels, height, width)
        features = self.encoder(flat_frames)  # [B*T, feature_channels, H, W]

        # æ¢å¤æ—¶åºç»´åº¦
        feature_channels = features.shape[1]
        features = features.view(batch_size, seq_len, feature_channels, height, width)

        # 2. æ—¶é—´æˆ³ (å‡è®¾ç­‰é—´éš”)
        times = torch.linspace(0, seq_len-1, seq_len)

        # 3. å‡†å¤‡ODEè¾“å…¥
        current_input = features[:, -1:, :, :, :]  # æœ€åä¸€å¸§ä½œä¸ºå½“å‰è¾“å…¥
        observations = features  # æ‰€æœ‰å¸§ä½œä¸ºè§‚æµ‹

        # 4. ç›®æ ‡æ—¶é—´ç‚¹
        if target_times is None:
            target_times = torch.linspace(seq_len, seq_len + self.num_future_frames - 1,
                                        self.num_future_frames)

        # 5. ODEé¢„æµ‹
        try:
            final_state, ode_loss, ode_predictions = self.ode_predictor(
                times=times,
                input=current_input,
                obs=observations,
                delta_t=0.1,
                T=target_times
            )

            # 6. è§£ç åˆ°åƒç´ ç©ºé—´
            # ode_predictions: [B, T_future, feature_channels, H, W]
            batch_size, future_len = ode_predictions.shape[:2]
            flat_predictions = ode_predictions.view(-1, feature_channels, height, width)
            decoded_frames = self.decoder(flat_predictions)

            # æ¢å¤å½¢çŠ¶
            predicted_frames = decoded_frames.view(batch_size, future_len, channels, height, width)

            return predicted_frames, ode_loss

        except Exception as e:
            print(f"ODEé¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›é›¶é¢„æµ‹ä½œä¸ºåå¤‡
            predicted_frames = torch.zeros(batch_size, len(target_times), channels, height, width)
            return predicted_frames, 0.0

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = MyVideoModelWithODE(input_channels=3, feature_channels=64, num_future_frames=4)

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 1
    seq_len = 5
    video_input = torch.randn(batch_size, seq_len, 3, 64, 64)

    # é¢„æµ‹
    predicted_frames, loss = model(video_input)
    print(f"è¾“å…¥å½¢çŠ¶: {video_input.shape}")
    print(f"é¢„æµ‹å½¢çŠ¶: {predicted_frames.shape}")
    print(f"ODEæŸå¤±: {loss}")
```

## ğŸ”§ 3. é«˜çº§ç”¨æ³•å’Œè‡ªå®šä¹‰

### 3.1 è‡ªå®šä¹‰æ—¶é—´åºåˆ—æ•°æ®

```python
class TimeSeriesODEModel(nn.Module):
    """æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ç¤ºä¾‹"""

    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()

        # å°†1Dæ—¶é—´åºåˆ—æ˜ å°„åˆ°2Dç‰¹å¾å›¾
        self.feature_mapper = nn.Linear(input_dim, hidden_dim * 4 * 4)

        # ODEé…ç½®
        cfg = create_custom_ode_config(
            out_channels=hidden_dim,
            latent_dim=hidden_dim,
            solver="midpoint",  # æ›´ç²¾ç¡®çš„æ±‚è§£å™¨
            delta_t=0.02
        )

        self.ode_model = NNFOwithBayesianJumps(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            cfg=cfg
        )

        # è¾“å‡ºæ˜ å°„
        self.output_mapper = nn.Linear(hidden_dim * 4 * 4, input_dim)
        self.hidden_dim = hidden_dim

    def forward(self, time_series, timestamps, target_timestamps):
        """
        Args:
            time_series: [B, T, input_dim] æ—¶é—´åºåˆ—æ•°æ®
            timestamps: [T] è§‚æµ‹æ—¶é—´æˆ³
            target_timestamps: [T_future] é¢„æµ‹æ—¶é—´æˆ³
        """
        batch_size, seq_len, input_dim = time_series.shape

        # æ˜ å°„åˆ°2Dç‰¹å¾å›¾
        features_flat = self.feature_mapper(time_series.view(-1, input_dim))
        features_2d = features_flat.view(batch_size, seq_len, self.hidden_dim, 4, 4)

        # ODEé¢„æµ‹
        current_input = features_2d[:, -1:, :, :, :]

        final_state, loss, predictions = self.ode_model(
            times=timestamps,
            input=current_input,
            obs=features_2d,
            delta_t=0.02,
            T=target_timestamps
        )

        # æ˜ å°„å›1D
        pred_flat = predictions.view(-1, self.hidden_dim * 4 * 4)
        output_series = self.output_mapper(pred_flat)
        output_series = output_series.view(batch_size, len(target_timestamps), input_dim)

        return output_series, loss

# ä½¿ç”¨ç¤ºä¾‹
ts_model = TimeSeriesODEModel(input_dim=10, hidden_dim=32)
time_series = torch.randn(2, 8, 10)  # [batch, time, features]
timestamps = torch.linspace(0, 7, 8)
target_timestamps = torch.linspace(8, 11, 4)

predictions, loss = ts_model(time_series, timestamps, target_timestamps)
print(f"æ—¶é—´åºåˆ—é¢„æµ‹å½¢çŠ¶: {predictions.shape}")  # [2, 4, 10]
```

### 3.2 å¤šæ¨¡æ€èåˆç¤ºä¾‹

```python
class MultimodalODEModel(nn.Module):
    """å¤šæ¨¡æ€æ•°æ®èåˆçš„ODEæ¨¡å‹"""

    def __init__(self, camera_channels=64, lidar_channels=64, fusion_channels=128):
        super().__init__()

        # æ¨¡æ€ç‰¹å®šç¼–ç å™¨
        self.camera_encoder = nn.Conv2d(camera_channels, fusion_channels//2, 1)
        self.lidar_encoder = nn.Conv2d(lidar_channels, fusion_channels//2, 1)

        # èåˆå±‚
        self.fusion_layer = nn.Conv2d(fusion_channels, fusion_channels, 3, padding=1)

        # åˆ›å»ºé…ç½®
        cfg = create_custom_ode_config(
            out_channels=fusion_channels,
            latent_dim=fusion_channels,
            solver="euler",
            delta_t=0.05
        )

        # æœªæ¥é¢„æµ‹ODE (æ›´é€‚åˆå¤šæ¨¡æ€åœºæ™¯)
        self.future_predictor = FuturePredictionODE(
            in_channels=fusion_channels,
            latent_dim=fusion_channels,
            cfg=cfg,
            n_gru_blocks=3,  # æ›´å¤šGRUå±‚å¤„ç†å¤æ‚ç‰¹å¾
            delta_t=0.05
        )

    def forward(self, camera_features, lidar_features, camera_timestamps,
                lidar_timestamps, target_timestamps):
        """
        Args:
            camera_features: [B, T_cam, C_cam, H, W]
            lidar_features: [B, T_lidar, C_lidar, H, W]
            camera_timestamps: [T_cam]
            lidar_timestamps: [T_lidar]
            target_timestamps: [T_future]
        """
        # ç¼–ç å„æ¨¡æ€ç‰¹å¾
        batch_size = camera_features.shape[0]

        # å¤„ç†ç›¸æœºç‰¹å¾
        camera_flat = camera_features.view(-1, *camera_features.shape[2:])
        camera_encoded = self.camera_encoder(camera_flat)
        camera_encoded = camera_encoded.view(*camera_features.shape[:2], *camera_encoded.shape[1:])

        # å¤„ç†æ¿€å…‰é›·è¾¾ç‰¹å¾
        lidar_flat = lidar_features.view(-1, *lidar_features.shape[2:])
        lidar_encoded = self.lidar_encoder(lidar_flat)
        lidar_encoded = lidar_encoded.view(*lidar_features.shape[:2], *lidar_encoded.shape[1:])

        # å½“å‰èåˆç‰¹å¾ (å‡è®¾æœ€æ–°çš„ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ç‰¹å¾)
        current_camera = camera_encoded[:, -1:, :, :, :]
        current_lidar = lidar_encoded[:, -1:, :, :, :]
        current_fused = torch.cat([current_camera, current_lidar], dim=2)
        current_fused = self.fusion_layer(current_fused.squeeze(1)).unsqueeze(1)

        # æœªæ¥é¢„æµ‹
        predictions, loss = self.future_predictor(
            future_prediction_input=current_fused,
            camera_states=camera_encoded,
            lidar_states=lidar_encoded,
            camera_timestamp=camera_timestamps.unsqueeze(0).repeat(batch_size, 1),
            lidar_timestamp=lidar_timestamps.unsqueeze(0).repeat(batch_size, 1),
            target_timestamp=target_timestamps.unsqueeze(0).repeat(batch_size, 1)
        )

        return predictions, loss
```

## ğŸ› ï¸ 4. é…ç½®å’Œä¼˜åŒ–

### 4.1 æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
# å¿«é€Ÿé…ç½® (ç”¨äºå®æ—¶åº”ç”¨)
fast_cfg = create_custom_ode_config(
    out_channels=32,          # è¾ƒå°çš„é€šé“æ•°
    latent_dim=32,
    solver="euler",           # æœ€å¿«çš„æ±‚è§£å™¨
    delta_t=0.1,             # è¾ƒå¤§çš„æ—¶é—´æ­¥
    use_variable_step=False   # å›ºå®šæ­¥é•¿æ›´å¿«
)

# ç²¾ç¡®é…ç½® (ç”¨äºç ”ç©¶/ç¦»çº¿å¤„ç†)
accurate_cfg = create_custom_ode_config(
    out_channels=128,         # æ›´å¤§çš„é€šé“æ•°
    latent_dim=128,
    solver="midpoint",        # æ›´ç²¾ç¡®çš„æ±‚è§£å™¨
    delta_t=0.01,            # æ›´å°çš„æ—¶é—´æ­¥
    use_variable_step=True    # è‡ªé€‚åº”æ­¥é•¿
)

# å¹³è¡¡é…ç½® (æ¨è)
balanced_cfg = create_custom_ode_config(
    out_channels=64,
    latent_dim=64,
    solver="euler",
    delta_t=0.05,
    use_variable_step=False
)
```

### 4.2 GPUå†…å­˜ä¼˜åŒ–

```python
import torch.utils.checkpoint as checkpoint

class MemoryEfficientODEModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.ode_model = NNFOwithBayesianJumps(64, 64, cfg)

    def forward(self, *args, **kwargs):
        # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
        return checkpoint.checkpoint(self.ode_model, *args, **kwargs)

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

model = MemoryEfficientODEModel(cfg)
scaler = GradScaler()

with autocast():
    predictions, loss = model(times, input_data, obs, delta_t, target_times)
```

## ğŸ“Š 5. æµ‹è¯•å’ŒéªŒè¯

### 5.1 åŸºç¡€åŠŸèƒ½æµ‹è¯•

```python
def test_ode_integration():
    """æµ‹è¯•ODEæ¨¡å—åŸºç¡€åŠŸèƒ½"""
    cfg = create_minimal_ode_config()
    model = NNFOwithBayesianJumps(64, 64, cfg)

    # æµ‹è¯•æ•°æ®
    batch_size = 2
    current_input = torch.randn(batch_size, 1, 64, 32, 32)
    observations = torch.randn(batch_size, 3, 64, 32, 32)
    times = torch.tensor([0.0, 0.5, 1.0])
    target_times = torch.tensor([1.5, 2.0])

    # å‰å‘ä¼ æ’­
    try:
        final_state, loss, predictions = model(
            times=times,
            input=current_input,
            obs=observations,
            delta_t=0.1,
            T=target_times
        )

        print("âœ… ODEæ¨¡å—æµ‹è¯•é€šè¿‡")
        print(f"é¢„æµ‹å½¢çŠ¶: {predictions.shape}")
        print(f"æŸå¤±å€¼: {loss}")
        return True

    except Exception as e:
        print(f"âŒ ODEæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_ode_integration()
```

## ğŸš¨ 6. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 6.1 å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜

```python
# é—®é¢˜: è¾“å…¥å½¢çŠ¶é”™è¯¯
# è§£å†³: ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®

# é”™è¯¯ç¤ºä¾‹
# input_wrong = torch.randn(2, 64, 32, 32)  # ç¼ºå°‘æ—¶åºç»´åº¦

# æ­£ç¡®ç¤ºä¾‹
input_correct = torch.randn(2, 1, 64, 32, 32)  # [B, 1, C, H, W]
obs_correct = torch.randn(2, 3, 64, 32, 32)    # [B, T, C, H, W]
```

### 6.2 é…ç½®å…¼å®¹æ€§é—®é¢˜

```python
# ä½¿ç”¨é…ç½®éªŒè¯å·¥å…·
from ode_modules.configs.config_validator import validate_ode_config

cfg = create_custom_ode_config(out_channels=128, latent_dim=64)  # ä¸åŒ¹é…!
is_valid, missing, warnings = validate_ode_config(cfg)

if warnings:
    print("âš ï¸ é…ç½®è­¦å‘Š:")
    for warning in warnings:
        print(f"  - {warning}")
```

### 6.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# 1. æ‰¹å¤„ç†ä¼˜åŒ–
def batch_predict(model, inputs, batch_size=4):
    """æ‰¹é‡é¢„æµ‹å‡å°‘GPUå†…å­˜å ç”¨"""
    results = []
    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i+batch_size]
        with torch.no_grad():
            pred = model(batch)
            results.append(pred)
    return torch.cat(results, dim=0)

# 2. ç¼“å­˜é‡ç”¨
@lru_cache(maxsize=128)
def get_ode_model(channels, solver):
    """ç¼“å­˜æ¨¡å‹é¿å…é‡å¤åˆ›å»º"""
    cfg = create_custom_ode_config(out_channels=channels, solver=solver)
    return NNFOwithBayesianJumps(channels, channels, cfg)
```

è¿™ä¸ªæŒ‡å—æ¶µç›–äº†åœ¨å…¶ä»–ç³»ç»Ÿä¸­ä½¿ç”¨ODEæ¨¡å—çš„æ‰€æœ‰å…³é”®æ–¹é¢ã€‚ä½ å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„é›†æˆæ–¹å¼ï¼