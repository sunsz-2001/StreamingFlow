"""
ODEæ¨¡å—è¾“å‡ºé€‚é…å™¨

è¿™ä¸ªæ–‡ä»¶åŒ…å«å°†ODEæ¨¡å—è¾“å‡ºè½¬æ¢ä¸ºå„ç§ç›®æ ‡æ ¼å¼çš„å‡½æ•°ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class OutputAdapter:
    """è¾“å‡ºé€‚é…å™¨åŸºç±»"""

    @staticmethod
    def adapt_output(ode_output, target_format):
        """é€‚é…ODEè¾“å‡ºåˆ°ç›®æ ‡æ ¼å¼"""
        raise NotImplementedError


class TimeSeriesOutputAdapter(OutputAdapter):
    """æ—¶é—´åºåˆ—è¾“å‡ºé€‚é…å™¨ - å°†2Dç‰¹å¾å›¾è½¬æ¢å›1Dæ—¶é—´åºåˆ—"""

    def __init__(self, feature_channels, feature_size, output_dim):
        """
        Args:
            feature_channels: ODEè¾“å‡ºçš„ç‰¹å¾é€šé“æ•°
            feature_size: ç‰¹å¾å›¾ç©ºé—´å°ºå¯¸
            output_dim: ç›®æ ‡æ—¶é—´åºåˆ—ç»´åº¦
        """
        self.feature_channels = feature_channels
        self.feature_size = feature_size
        self.output_dim = output_dim

        # é€†æ˜ å°„ç½‘ç»œ
        self.inverse_mapper = nn.Sequential(
            nn.Linear(feature_channels * feature_size * feature_size, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, output_dim)
        )

    def adapt_output(self, ode_predictions, original_timestamps=None):
        """
        å°†ODEé¢„æµ‹è½¬æ¢ä¸ºæ—¶é—´åºåˆ—

        Args:
            ode_predictions: [B, T_future, C, H, W] ODEé¢„æµ‹ç»“æœ
            original_timestamps: [T_future] é¢„æµ‹æ—¶é—´æˆ³

        Returns:
            dict: åŒ…å«é€‚é…åçš„è¾“å‡º
        """
        batch_size, seq_len, channels, height, width = ode_predictions.shape

        # 1. å±•å¹³ç‰¹å¾å›¾
        flat_features = ode_predictions.view(batch_size * seq_len, -1)

        # 2. æ˜ å°„å›æ—¶é—´åºåˆ—
        time_series = self.inverse_mapper(flat_features)

        # 3. æ¢å¤æ—¶åºç»´åº¦
        time_series = time_series.view(batch_size, seq_len, self.output_dim)

        result = {
            'predictions': time_series,  # [B, T_future, output_dim]
            'timestamps': original_timestamps
        }

        # 4. è®¡ç®—é¢„æµ‹ç»Ÿè®¡
        result['statistics'] = {
            'mean': time_series.mean(dim=(0, 1)),
            'std': time_series.std(dim=(0, 1)),
            'min': time_series.min(dim=1)[0].min(dim=0)[0],
            'max': time_series.max(dim=1)[0].max(dim=0)[0]
        }

        return result


class ImageSequenceOutputAdapter(OutputAdapter):
    """å›¾åƒåºåˆ—è¾“å‡ºé€‚é…å™¨ - å°†ç‰¹å¾å›¾è½¬æ¢å›å›¾åƒ"""

    def __init__(self, feature_channels, target_size=(128, 128), target_channels=3):
        """
        Args:
            feature_channels: ODEè¾“å‡ºçš„ç‰¹å¾é€šé“æ•°
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            target_channels: ç›®æ ‡å›¾åƒé€šé“æ•° (å¦‚RGB=3)
        """
        self.feature_channels = feature_channels
        self.target_size = target_size
        self.target_channels = target_channels

        # å›¾åƒé‡å»ºç½‘ç»œ
        self.decoder = nn.Sequential(
            nn.Conv2d(feature_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, target_channels, 3, padding=1),
            nn.Sigmoid()  # å‡è®¾è¾“å‡ºåœ¨[0,1]èŒƒå›´
        )

    def adapt_output(self, ode_predictions, original_timestamps=None):
        """
        å°†ODEé¢„æµ‹è½¬æ¢ä¸ºå›¾åƒåºåˆ—

        Args:
            ode_predictions: [B, T_future, C, H, W] ODEé¢„æµ‹ç»“æœ
            original_timestamps: [T_future] é¢„æµ‹æ—¶é—´æˆ³

        Returns:
            dict: åŒ…å«é€‚é…åçš„è¾“å‡º
        """
        batch_size, seq_len = ode_predictions.shape[:2]

        # 1. å±•å¹³æ—¶åºç»´åº¦
        flat_features = ode_predictions.view(-1, *ode_predictions.shape[2:])

        # 2. ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        if flat_features.shape[-2:] != self.target_size:
            upsampled = F.interpolate(flat_features, size=self.target_size,
                                    mode='bilinear', align_corners=False)
        else:
            upsampled = flat_features

        # 3. è§£ç ä¸ºå›¾åƒ
        decoded_images = self.decoder(upsampled)

        # 4. æ¢å¤æ—¶åºç»´åº¦
        image_sequence = decoded_images.view(batch_size, seq_len,
                                           self.target_channels, *self.target_size)

        result = {
            'predictions': image_sequence,  # [B, T_future, C, H, W]
            'timestamps': original_timestamps
        }

        # 5. è®¡ç®—å›¾åƒè´¨é‡æŒ‡æ ‡
        result['quality_metrics'] = {
            'mean_intensity': image_sequence.mean(),
            'std_intensity': image_sequence.std(),
            'spatial_gradient': self._compute_spatial_gradient(image_sequence)
        }

        return result

    def _compute_spatial_gradient(self, images):
        """è®¡ç®—ç©ºé—´æ¢¯åº¦ä½œä¸ºå›¾åƒæ¸…æ™°åº¦æŒ‡æ ‡"""
        # Sobelç®—å­
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                              dtype=images.dtype, device=images.device).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
                              dtype=images.dtype, device=images.device).view(1, 1, 3, 3)

        # å¯¹æ¯ä¸ªé€šé“è®¡ç®—æ¢¯åº¦
        grad_x = F.conv2d(images.view(-1, 1, *images.shape[-2:]), sobel_x, padding=1)
        grad_y = F.conv2d(images.view(-1, 1, *images.shape[-2:]), sobel_y, padding=1)

        # æ¢¯åº¦å¹…åº¦
        gradient_magnitude = torch.sqrt(grad_x**2 + grad_y**2)
        return gradient_magnitude.mean()


class PointCloudOutputAdapter(OutputAdapter):
    """ç‚¹äº‘è¾“å‡ºé€‚é…å™¨ - å°†BEVç‰¹å¾å›¾è½¬æ¢ä¸ºç‚¹äº‘"""

    def __init__(self, feature_channels, grid_size=(64, 64),
                 x_range=(-50, 50), y_range=(-50, 50), max_points=1000):
        """
        Args:
            feature_channels: è¾“å…¥ç‰¹å¾é€šé“æ•°
            grid_size: BEVç½‘æ ¼å°ºå¯¸
            x_range: Xè½´èŒƒå›´
            y_range: Yè½´èŒƒå›´
            max_points: æœ€å¤§ç‚¹æ•°
        """
        self.feature_channels = feature_channels
        self.grid_size = grid_size
        self.x_range = x_range
        self.y_range = y_range
        self.max_points = max_points

        # ç‚¹äº‘ç”Ÿæˆç½‘ç»œ
        self.point_generator = nn.Sequential(
            nn.Conv2d(feature_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 4, 1),  # è¾“å‡º [x, y, z, intensity]
            nn.Tanh()  # å½’ä¸€åŒ–è¾“å‡º
        )

    def adapt_output(self, ode_predictions, original_timestamps=None):
        """
        å°†ODEé¢„æµ‹è½¬æ¢ä¸ºç‚¹äº‘åºåˆ—

        Args:
            ode_predictions: [B, T_future, C, H, W] ODEé¢„æµ‹ç»“æœ
            original_timestamps: [T_future] é¢„æµ‹æ—¶é—´æˆ³

        Returns:
            dict: åŒ…å«é€‚é…åçš„è¾“å‡º
        """
        batch_size, seq_len = ode_predictions.shape[:2]

        point_clouds = []

        for t in range(seq_len):
            batch_points = []

            for b in range(batch_size):
                # 1. ç”Ÿæˆç‚¹äº‘å‚æ•°
                bev_features = ode_predictions[b, t]  # [C, H, W]
                point_params = self.point_generator(bev_features.unsqueeze(0))  # [1, 4, H, W]
                point_params = point_params.squeeze(0)  # [4, H, W]

                # 2. è½¬æ¢ä¸ºç‚¹äº‘
                points = self._bev_to_points(point_params)
                batch_points.append(points)

            point_clouds.append(batch_points)

        result = {
            'predictions': point_clouds,  # List[List[Tensor]] [T_future][B][N, 4]
            'timestamps': original_timestamps
        }

        # 3. è®¡ç®—ç‚¹äº‘ç»Ÿè®¡
        result['statistics'] = self._compute_pointcloud_stats(point_clouds)

        return result

    def _bev_to_points(self, point_params):
        """å°†BEVç‰¹å¾è½¬æ¢ä¸ºç‚¹äº‘"""
        _, h, w = point_params.shape

        # åˆ›å»ºç½‘æ ¼åæ ‡
        y_indices, x_indices = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
        y_indices = y_indices.to(point_params.device).float()
        x_indices = x_indices.to(point_params.device).float()

        # è½¬æ¢ä¸ºä¸–ç•Œåæ ‡
        x_world = (x_indices / (w - 1)) * (self.x_range[1] - self.x_range[0]) + self.x_range[0]
        y_world = (y_indices / (h - 1)) * (self.y_range[1] - self.y_range[0]) + self.y_range[0]

        # æå–ç‚¹äº‘å±æ€§
        x_offset = point_params[0] * 2.0  # ä½ç½®åç§»
        y_offset = point_params[1] * 2.0
        z_height = point_params[2] * 5.0  # é«˜åº¦
        intensity = torch.sigmoid(point_params[3])  # å¼ºåº¦

        # åº”ç”¨åç§»
        x_final = x_world + x_offset
        y_final = y_world + y_offset

        # ç»„è£…ç‚¹äº‘
        points = torch.stack([
            x_final.flatten(),
            y_final.flatten(),
            z_height.flatten(),
            intensity.flatten()
        ], dim=1)  # [H*W, 4]

        # è¿‡æ»¤æœ‰æ•ˆç‚¹ (åŸºäºå¼ºåº¦é˜ˆå€¼)
        valid_mask = intensity.flatten() > 0.1
        valid_points = points[valid_mask]

        # é™åˆ¶ç‚¹æ•°
        if len(valid_points) > self.max_points:
            indices = torch.randperm(len(valid_points))[:self.max_points]
            valid_points = valid_points[indices]

        return valid_points

    def _compute_pointcloud_stats(self, point_clouds):
        """è®¡ç®—ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯"""
        all_points = []
        for t_points in point_clouds:
            for b_points in t_points:
                all_points.append(b_points)

        if not all_points:
            return {}

        concatenated = torch.cat(all_points, dim=0)

        return {
            'total_points': len(concatenated),
            'mean_position': concatenated[:, :3].mean(dim=0),
            'std_position': concatenated[:, :3].std(dim=0),
            'mean_intensity': concatenated[:, 3].mean(),
            'point_density': len(concatenated) / len(all_points)
        }


class SegmentationOutputAdapter(OutputAdapter):
    """è¯­ä¹‰åˆ†å‰²è¾“å‡ºé€‚é…å™¨"""

    def __init__(self, feature_channels, num_classes, target_size=(256, 256)):
        """
        Args:
            feature_channels: è¾“å…¥ç‰¹å¾é€šé“æ•°
            num_classes: åˆ†å‰²ç±»åˆ«æ•°
            target_size: ç›®æ ‡åˆ†å‰²å›¾å°ºå¯¸
        """
        self.feature_channels = feature_channels
        self.num_classes = num_classes
        self.target_size = target_size

        # åˆ†å‰²å¤´
        self.seg_head = nn.Sequential(
            nn.Conv2d(feature_channels, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, num_classes, 1)
        )

    def adapt_output(self, ode_predictions, original_timestamps=None):
        """
        å°†ODEé¢„æµ‹è½¬æ¢ä¸ºåˆ†å‰²å›¾åºåˆ—

        Args:
            ode_predictions: [B, T_future, C, H, W] ODEé¢„æµ‹ç»“æœ
            original_timestamps: [T_future] é¢„æµ‹æ—¶é—´æˆ³

        Returns:
            dict: åŒ…å«é€‚é…åçš„è¾“å‡º
        """
        batch_size, seq_len = ode_predictions.shape[:2]

        # 1. å±•å¹³æ—¶åºç»´åº¦
        flat_features = ode_predictions.view(-1, *ode_predictions.shape[2:])

        # 2. ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        if flat_features.shape[-2:] != self.target_size:
            upsampled = F.interpolate(flat_features, size=self.target_size,
                                    mode='bilinear', align_corners=False)
        else:
            upsampled = flat_features

        # 3. ç”Ÿæˆåˆ†å‰²å›¾
        seg_logits = self.seg_head(upsampled)

        # 4. æ¢å¤æ—¶åºç»´åº¦
        seg_logits = seg_logits.view(batch_size, seq_len, self.num_classes, *self.target_size)

        # 5. è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        seg_probs = F.softmax(seg_logits, dim=2)
        seg_predictions = torch.argmax(seg_logits, dim=2)

        result = {
            'logits': seg_logits,          # [B, T_future, num_classes, H, W]
            'probabilities': seg_probs,     # [B, T_future, num_classes, H, W]
            'predictions': seg_predictions, # [B, T_future, H, W]
            'timestamps': original_timestamps
        }

        # 6. è®¡ç®—åˆ†å‰²è´¨é‡æŒ‡æ ‡
        result['quality_metrics'] = {
            'confidence': seg_probs.max(dim=2)[0].mean(),
            'entropy': self._compute_entropy(seg_probs),
            'class_distribution': self._compute_class_distribution(seg_predictions)
        }

        return result

    def _compute_entropy(self, probs):
        """è®¡ç®—é¢„æµ‹ç†µ (ä¸ç¡®å®šæ€§æŒ‡æ ‡)"""
        log_probs = torch.log(probs + 1e-8)
        entropy = -(probs * log_probs).sum(dim=2)
        return entropy.mean()

    def _compute_class_distribution(self, predictions):
        """è®¡ç®—ç±»åˆ«åˆ†å¸ƒ"""
        class_counts = torch.zeros(self.num_classes, device=predictions.device)
        for c in range(self.num_classes):
            class_counts[c] = (predictions == c).sum().float()
        return class_counts / predictions.numel()


# ä¾¿æ·çš„é€‚é…å™¨å·¥å‚å‡½æ•°
def create_output_adapter(output_type, **kwargs):
    """
    æ ¹æ®è¾“å‡ºç±»å‹åˆ›å»ºé€‚é…å™¨

    Args:
        output_type: 'timeseries', 'images', 'pointcloud', 'segmentation'
        **kwargs: é€‚é…å™¨å‚æ•°

    Returns:
        é€‚é…å™¨å®ä¾‹
    """
    adapters = {
        'timeseries': TimeSeriesOutputAdapter,
        'images': ImageSequenceOutputAdapter,
        'pointcloud': PointCloudOutputAdapter,
        'segmentation': SegmentationOutputAdapter
    }

    if output_type not in adapters:
        raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºç±»å‹: {output_type}")

    return adapters[output_type](**kwargs)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ¯ ODEæ¨¡å—è¾“å‡ºé€‚é…å™¨ç¤ºä¾‹")

    # æ¨¡æ‹ŸODEè¾“å‡º
    ode_output = torch.randn(2, 3, 64, 32, 32)  # [B, T_future, C, H, W]

    # æ—¶é—´åºåˆ—é€‚é…
    ts_adapter = TimeSeriesOutputAdapter(
        feature_channels=64, feature_size=32, output_dim=10
    )
    ts_result = ts_adapter.adapt_output(ode_output)
    print(f"æ—¶é—´åºåˆ—è¾“å‡º: {ts_result['predictions'].shape}")

    # å›¾åƒåºåˆ—é€‚é…
    img_adapter = ImageSequenceOutputAdapter(
        feature_channels=64, target_size=(128, 128)
    )
    img_result = img_adapter.adapt_output(ode_output)
    print(f"å›¾åƒåºåˆ—è¾“å‡º: {img_result['predictions'].shape}")

    print("âœ… è¾“å‡ºé€‚é…å™¨åˆ›å»ºæˆåŠŸ!")