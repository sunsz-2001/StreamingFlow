"""
ODEæ¨¡å—åŒ…è£…å™¨ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•å°†è¾“å…¥é€‚é…å™¨ã€ODEæ¨¡å—å’Œè¾“å‡ºé€‚é…å™¨ç»„åˆæˆå®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚
"""

import torch
import torch.nn as nn
from typing import Union, List, Dict, Any, Optional

# å¯¼å…¥é€‚é…å™¨
from .input_adapters import (
    TimeSeriesAdapter, ImageSequenceAdapter, PointCloudAdapter, MultiModalAdapter
)
from .output_adapters import (
    TimeSeriesOutputAdapter, ImageSequenceOutputAdapter,
    PointCloudOutputAdapter, SegmentationOutputAdapter
)

# å¯¼å…¥ODEæ¨¡å—
try:
    from .. import NNFOwithBayesianJumps, FuturePredictionODE
    from ..configs.minimal_ode_config import create_custom_ode_config
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è·¯å¾„
    pass


class ODEWrapper(nn.Module):
    """ODEæ¨¡å—åŒ…è£…å™¨åŸºç±»"""

    def __init__(self, ode_model, input_adapter, output_adapter):
        """
        Args:
            ode_model: ODEæ¨¡å‹å®ä¾‹
            input_adapter: è¾“å…¥é€‚é…å™¨
            output_adapter: è¾“å‡ºé€‚é…å™¨
        """
        super().__init__()
        self.ode_model = ode_model
        self.input_adapter = input_adapter
        self.output_adapter = output_adapter

    def forward(self, *args, **kwargs):
        """å­ç±»éœ€è¦å®ç°å…·ä½“çš„å‰å‘ä¼ æ’­é€»è¾‘"""
        raise NotImplementedError


class TimeSeriesODEWrapper(ODEWrapper):
    """æ—¶é—´åºåˆ—ODEåŒ…è£…å™¨"""

    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 64,
                 feature_size: int = 32, **ode_kwargs):
        """
        Args:
            input_dim: è¾“å…¥æ—¶é—´åºåˆ—ç»´åº¦
            output_dim: è¾“å‡ºæ—¶é—´åºåˆ—ç»´åº¦
            hidden_dim: ODEéšè—å±‚ç»´åº¦
            feature_size: ä¸­é—´ç‰¹å¾å›¾å°ºå¯¸
            **ode_kwargs: ODEé…ç½®å‚æ•°
        """
        # åˆ›å»ºé€‚é…å™¨
        input_adapter = TimeSeriesAdapter(
            input_dim=input_dim,
            output_channels=hidden_dim,
            feature_size=feature_size
        )

        output_adapter = TimeSeriesOutputAdapter(
            feature_channels=hidden_dim,
            feature_size=feature_size,
            output_dim=output_dim
        )

        # åˆ›å»ºODEé…ç½®
        cfg = create_custom_ode_config(
            out_channels=hidden_dim,
            latent_dim=hidden_dim,
            **ode_kwargs
        )

        # åˆ›å»ºODEæ¨¡å‹
        ode_model = NNFOwithBayesianJumps(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            cfg=cfg
        )

        super().__init__(ode_model, input_adapter, output_adapter)

    def forward(self, time_series: torch.Tensor,
                timestamps: Optional[torch.Tensor] = None,
                target_times: Optional[torch.Tensor] = None,
                delta_t: float = 0.1) -> Dict[str, Any]:
        """
        æ—¶é—´åºåˆ—é¢„æµ‹

        Args:
            time_series: [B, T, input_dim] è¾“å…¥æ—¶é—´åºåˆ—
            timestamps: [T] è§‚æµ‹æ—¶é—´æˆ³
            target_times: [T_future] é¢„æµ‹æ—¶é—´æˆ³
            delta_t: ç§¯åˆ†æ­¥é•¿

        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œç›¸å…³ä¿¡æ¯
        """
        # 1. è¾“å…¥é€‚é…
        adapted_input = self.input_adapter.adapt_input(time_series, timestamps)

        # 2. ç”Ÿæˆé»˜è®¤ç›®æ ‡æ—¶é—´
        if target_times is None:
            seq_len = time_series.shape[1]
            target_times = torch.linspace(seq_len, seq_len + 3, 4, dtype=torch.float32)

        # 3. ODEé¢„æµ‹
        final_state, ode_loss, ode_predictions = self.ode_model(
            times=adapted_input['times'],
            input=adapted_input['current_input'],
            obs=adapted_input['observations'],
            delta_t=delta_t,
            T=target_times
        )

        # 4. è¾“å‡ºé€‚é…
        output_result = self.output_adapter.adapt_output(ode_predictions, target_times)

        # 5. ç»„åˆç»“æœ
        result = {
            'predictions': output_result['predictions'],
            'timestamps': target_times,
            'ode_loss': ode_loss,
            'statistics': output_result.get('statistics', {}),
            'raw_ode_output': ode_predictions
        }

        return result


class VideoODEWrapper(ODEWrapper):
    """è§†é¢‘é¢„æµ‹ODEåŒ…è£…å™¨"""

    def __init__(self, input_channels: int = 3, feature_channels: int = 64,
                 target_size: tuple = (128, 128), **ode_kwargs):
        """
        Args:
            input_channels: è¾“å…¥è§†é¢‘é€šé“æ•°
            feature_channels: ç‰¹å¾é€šé“æ•°
            target_size: ç›®æ ‡è§†é¢‘å°ºå¯¸
            **ode_kwargs: ODEé…ç½®å‚æ•°
        """
        # åˆ›å»ºé€‚é…å™¨
        input_adapter = ImageSequenceAdapter(
            target_size=target_size,
            target_channels=feature_channels
        )

        output_adapter = ImageSequenceOutputAdapter(
            feature_channels=feature_channels,
            target_size=target_size,
            target_channels=input_channels
        )

        # åˆ›å»ºODEé…ç½®
        cfg = create_custom_ode_config(
            out_channels=feature_channels,
            latent_dim=feature_channels,
            **ode_kwargs
        )

        # åˆ›å»ºODEæ¨¡å‹
        ode_model = NNFOwithBayesianJumps(
            input_size=feature_channels,
            hidden_size=feature_channels,
            cfg=cfg
        )

        super().__init__(ode_model, input_adapter, output_adapter)

    def forward(self, video_sequence: torch.Tensor,
                timestamps: Optional[torch.Tensor] = None,
                target_times: Optional[torch.Tensor] = None,
                delta_t: float = 0.1) -> Dict[str, Any]:
        """
        è§†é¢‘åºåˆ—é¢„æµ‹

        Args:
            video_sequence: [B, T, C, H, W] è¾“å…¥è§†é¢‘åºåˆ—
            timestamps: [T] è§‚æµ‹æ—¶é—´æˆ³
            target_times: [T_future] é¢„æµ‹æ—¶é—´æˆ³
            delta_t: ç§¯åˆ†æ­¥é•¿

        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œç›¸å…³ä¿¡æ¯
        """
        # 1. è¾“å…¥é€‚é…
        adapted_input = self.input_adapter.adapt_input(video_sequence, timestamps)

        # 2. ç”Ÿæˆé»˜è®¤ç›®æ ‡æ—¶é—´
        if target_times is None:
            seq_len = video_sequence.shape[1]
            target_times = torch.linspace(seq_len, seq_len + 2, 3, dtype=torch.float32)

        # 3. ODEé¢„æµ‹
        final_state, ode_loss, ode_predictions = self.ode_model(
            times=adapted_input['times'],
            input=adapted_input['current_input'],
            obs=adapted_input['observations'],
            delta_t=delta_t,
            T=target_times
        )

        # 4. è¾“å‡ºé€‚é…
        output_result = self.output_adapter.adapt_output(ode_predictions, target_times)

        # 5. ç»„åˆç»“æœ
        result = {
            'predictions': output_result['predictions'],
            'timestamps': target_times,
            'ode_loss': ode_loss,
            'quality_metrics': output_result.get('quality_metrics', {}),
            'raw_ode_output': ode_predictions
        }

        return result


class LidarODEWrapper(ODEWrapper):
    """æ¿€å…‰é›·è¾¾ç‚¹äº‘ODEåŒ…è£…å™¨"""

    def __init__(self, grid_size: tuple = (64, 64), feature_channels: int = 64,
                 x_range: tuple = (-50, 50), y_range: tuple = (-50, 50),
                 max_points: int = 1000, **ode_kwargs):
        """
        Args:
            grid_size: BEVç½‘æ ¼å°ºå¯¸
            feature_channels: ç‰¹å¾é€šé“æ•°
            x_range: Xè½´èŒƒå›´
            y_range: Yè½´èŒƒå›´
            max_points: æœ€å¤§è¾“å‡ºç‚¹æ•°
            **ode_kwargs: ODEé…ç½®å‚æ•°
        """
        # åˆ›å»ºé€‚é…å™¨
        input_adapter = PointCloudAdapter(
            grid_size=grid_size,
            feature_channels=feature_channels,
            x_range=x_range,
            y_range=y_range
        )

        output_adapter = PointCloudOutputAdapter(
            feature_channels=feature_channels,
            grid_size=grid_size,
            x_range=x_range,
            y_range=y_range,
            max_points=max_points
        )

        # åˆ›å»ºODEé…ç½®
        cfg = create_custom_ode_config(
            out_channels=feature_channels,
            latent_dim=feature_channels,
            **ode_kwargs
        )

        # åˆ›å»ºODEæ¨¡å‹
        ode_model = NNFOwithBayesianJumps(
            input_size=feature_channels,
            hidden_size=feature_channels,
            cfg=cfg
        )

        super().__init__(ode_model, input_adapter, output_adapter)

    def forward(self, point_clouds: List[torch.Tensor],
                timestamps: Optional[torch.Tensor] = None,
                target_times: Optional[torch.Tensor] = None,
                delta_t: float = 0.1) -> Dict[str, Any]:
        """
        ç‚¹äº‘åºåˆ—é¢„æµ‹

        Args:
            point_clouds: List[Tensor] ç‚¹äº‘åºåˆ—ï¼Œæ¯ä¸ªç‚¹äº‘å½¢çŠ¶ä¸º[N, 4]
            timestamps: [T] è§‚æµ‹æ—¶é—´æˆ³
            target_times: [T_future] é¢„æµ‹æ—¶é—´æˆ³
            delta_t: ç§¯åˆ†æ­¥é•¿

        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œç›¸å…³ä¿¡æ¯
        """
        # 1. è¾“å…¥é€‚é…
        adapted_input = self.input_adapter.adapt_input(point_clouds, timestamps)

        # 2. ç”Ÿæˆé»˜è®¤ç›®æ ‡æ—¶é—´
        if target_times is None:
            seq_len = len(point_clouds)
            target_times = torch.linspace(seq_len, seq_len + 2, 3, dtype=torch.float32)

        # 3. ODEé¢„æµ‹
        final_state, ode_loss, ode_predictions = self.ode_model(
            times=adapted_input['times'],
            input=adapted_input['current_input'],
            obs=adapted_input['observations'],
            delta_t=delta_t,
            T=target_times
        )

        # 4. è¾“å‡ºé€‚é…
        output_result = self.output_adapter.adapt_output(ode_predictions, target_times)

        # 5. ç»„åˆç»“æœ
        result = {
            'predictions': output_result['predictions'],
            'timestamps': target_times,
            'ode_loss': ode_loss,
            'statistics': output_result.get('statistics', {}),
            'raw_ode_output': ode_predictions
        }

        return result


class SegmentationODEWrapper(ODEWrapper):
    """è¯­ä¹‰åˆ†å‰²ODEåŒ…è£…å™¨"""

    def __init__(self, input_channels: int = 3, feature_channels: int = 64,
                 num_classes: int = 21, target_size: tuple = (256, 256),
                 **ode_kwargs):
        """
        Args:
            input_channels: è¾“å…¥å›¾åƒé€šé“æ•°
            feature_channels: ç‰¹å¾é€šé“æ•°
            num_classes: åˆ†å‰²ç±»åˆ«æ•°
            target_size: ç›®æ ‡åˆ†å‰²å›¾å°ºå¯¸
            **ode_kwargs: ODEé…ç½®å‚æ•°
        """
        # åˆ›å»ºé€‚é…å™¨
        input_adapter = ImageSequenceAdapter(
            target_size=target_size,
            target_channels=feature_channels
        )

        output_adapter = SegmentationOutputAdapter(
            feature_channels=feature_channels,
            num_classes=num_classes,
            target_size=target_size
        )

        # åˆ›å»ºODEé…ç½®
        cfg = create_custom_ode_config(
            out_channels=feature_channels,
            latent_dim=feature_channels,
            **ode_kwargs
        )

        # åˆ›å»ºODEæ¨¡å‹
        ode_model = NNFOwithBayesianJumps(
            input_size=feature_channels,
            hidden_size=feature_channels,
            cfg=cfg
        )

        super().__init__(ode_model, input_adapter, output_adapter)
        self.num_classes = num_classes

    def forward(self, image_sequence: torch.Tensor,
                timestamps: Optional[torch.Tensor] = None,
                target_times: Optional[torch.Tensor] = None,
                delta_t: float = 0.1) -> Dict[str, Any]:
        """
        å›¾åƒåºåˆ—åˆ†å‰²é¢„æµ‹

        Args:
            image_sequence: [B, T, C, H, W] è¾“å…¥å›¾åƒåºåˆ—
            timestamps: [T] è§‚æµ‹æ—¶é—´æˆ³
            target_times: [T_future] é¢„æµ‹æ—¶é—´æˆ³
            delta_t: ç§¯åˆ†æ­¥é•¿

        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œç›¸å…³ä¿¡æ¯
        """
        # 1. è¾“å…¥é€‚é…
        adapted_input = self.input_adapter.adapt_input(image_sequence, timestamps)

        # 2. ç”Ÿæˆé»˜è®¤ç›®æ ‡æ—¶é—´
        if target_times is None:
            seq_len = image_sequence.shape[1]
            target_times = torch.linspace(seq_len, seq_len + 1, 2, dtype=torch.float32)

        # 3. ODEé¢„æµ‹
        final_state, ode_loss, ode_predictions = self.ode_model(
            times=adapted_input['times'],
            input=adapted_input['current_input'],
            obs=adapted_input['observations'],
            delta_t=delta_t,
            T=target_times
        )

        # 4. è¾“å‡ºé€‚é…
        output_result = self.output_adapter.adapt_output(ode_predictions, target_times)

        # 5. ç»„åˆç»“æœ
        result = {
            'logits': output_result['logits'],
            'probabilities': output_result['probabilities'],
            'predictions': output_result['predictions'],
            'timestamps': target_times,
            'ode_loss': ode_loss,
            'quality_metrics': output_result.get('quality_metrics', {}),
            'raw_ode_output': ode_predictions
        }

        return result


class CustomODEWrapper(ODEWrapper):
    """è‡ªå®šä¹‰ODEåŒ…è£…å™¨ - ç”¨æˆ·å¯ä»¥æä¾›è‡ªå·±çš„é€‚é…å™¨"""

    def __init__(self, input_adapter, output_adapter, ode_config=None, **ode_kwargs):
        """
        Args:
            input_adapter: ç”¨æˆ·è‡ªå®šä¹‰çš„è¾“å…¥é€‚é…å™¨
            output_adapter: ç”¨æˆ·è‡ªå®šä¹‰çš„è¾“å‡ºé€‚é…å™¨
            ode_config: ODEé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            **ode_kwargs: ODEé…ç½®å‚æ•°
        """
        # åˆ›å»ºODEé…ç½®
        if ode_config is None:
            ode_config = create_custom_ode_config(**ode_kwargs)

        # ä»é€‚é…å™¨æ¨æ–­é€šé“æ•°
        if hasattr(input_adapter, 'output_channels'):
            channels = input_adapter.output_channels
        elif hasattr(input_adapter, 'target_channels'):
            channels = input_adapter.target_channels
        else:
            channels = ode_kwargs.get('out_channels', 64)

        # åˆ›å»ºODEæ¨¡å‹
        ode_model = NNFOwithBayesianJumps(
            input_size=channels,
            hidden_size=channels,
            cfg=ode_config
        )

        super().__init__(ode_model, input_adapter, output_adapter)

    def forward(self, input_data, timestamps=None, target_times=None, delta_t=0.1, **kwargs):
        """
        é€šç”¨å‰å‘ä¼ æ’­

        Args:
            input_data: è¾“å…¥æ•°æ® (æ ¼å¼å–å†³äºè¾“å…¥é€‚é…å™¨)
            timestamps: è§‚æµ‹æ—¶é—´æˆ³
            target_times: é¢„æµ‹æ—¶é—´æˆ³
            delta_t: ç§¯åˆ†æ­¥é•¿
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        # 1. è¾“å…¥é€‚é…
        adapted_input = self.input_adapter.adapt_input(input_data, timestamps)

        # 2. ç”Ÿæˆé»˜è®¤ç›®æ ‡æ—¶é—´
        if target_times is None:
            if hasattr(input_data, 'shape') and len(input_data.shape) > 1:
                seq_len = input_data.shape[1]
            else:
                seq_len = len(input_data) if isinstance(input_data, (list, tuple)) else 5
            target_times = torch.linspace(seq_len, seq_len + 2, 3, dtype=torch.float32)

        # 3. ODEé¢„æµ‹
        final_state, ode_loss, ode_predictions = self.ode_model(
            times=adapted_input['times'],
            input=adapted_input['current_input'],
            obs=adapted_input['observations'],
            delta_t=delta_t,
            T=target_times
        )

        # 4. è¾“å‡ºé€‚é…
        output_result = self.output_adapter.adapt_output(ode_predictions, target_times)

        # 5. ç»„åˆç»“æœ
        result = output_result.copy()
        result.update({
            'timestamps': target_times,
            'ode_loss': ode_loss,
            'raw_ode_output': ode_predictions
        })

        return result


# ä¾¿æ·çš„åŒ…è£…å™¨å·¥å‚å‡½æ•°
def create_ode_wrapper(wrapper_type: str, **kwargs) -> ODEWrapper:
    """
    æ ¹æ®ç±»å‹åˆ›å»ºODEåŒ…è£…å™¨

    Args:
        wrapper_type: åŒ…è£…å™¨ç±»å‹
        **kwargs: åŒ…è£…å™¨å‚æ•°

    Returns:
        ODEWrapperå®ä¾‹
    """
    wrappers = {
        'timeseries': TimeSeriesODEWrapper,
        'video': VideoODEWrapper,
        'lidar': LidarODEWrapper,
        'segmentation': SegmentationODEWrapper,
        'custom': CustomODEWrapper
    }

    if wrapper_type not in wrappers:
        raise ValueError(f"ä¸æ”¯æŒçš„åŒ…è£…å™¨ç±»å‹: {wrapper_type}")

    return wrappers[wrapper_type](**kwargs)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ”§ ODEåŒ…è£…å™¨ä½¿ç”¨ç¤ºä¾‹")

    # æ—¶é—´åºåˆ—é¢„æµ‹ç¤ºä¾‹
    print("\n1. æ—¶é—´åºåˆ—é¢„æµ‹:")
    ts_wrapper = TimeSeriesODEWrapper(input_dim=10, output_dim=10, hidden_dim=32)
    time_series = torch.randn(2, 5, 10)  # [batch, time, features]

    with torch.no_grad():
        ts_result = ts_wrapper(time_series)
        print(f"   è¾“å…¥å½¢çŠ¶: {time_series.shape}")
        print(f"   é¢„æµ‹å½¢çŠ¶: {ts_result['predictions'].shape}")

    # è§†é¢‘é¢„æµ‹ç¤ºä¾‹
    print("\n2. è§†é¢‘é¢„æµ‹:")
    video_wrapper = VideoODEWrapper(feature_channels=32, target_size=(64, 64))
    video_data = torch.randn(1, 4, 3, 64, 64)  # [batch, time, channels, h, w]

    with torch.no_grad():
        video_result = video_wrapper(video_data)
        print(f"   è¾“å…¥å½¢çŠ¶: {video_data.shape}")
        print(f"   é¢„æµ‹å½¢çŠ¶: {video_result['predictions'].shape}")

    print("\nâœ… æ‰€æœ‰åŒ…è£…å™¨ç¤ºä¾‹å®Œæˆ!")