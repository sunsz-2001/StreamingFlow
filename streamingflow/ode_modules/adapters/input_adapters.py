"""
ODEæ¨¡å—è¾“å…¥é€‚é…å™¨

è¿™ä¸ªæ–‡ä»¶åŒ…å«å„ç§è¾“å…¥æ ¼å¼åˆ°ODEæ¨¡å—æ ‡å‡†æ ¼å¼çš„è½¬æ¢å‡½æ•°ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class InputAdapter:
    """è¾“å…¥é€‚é…å™¨åŸºç±»"""

    @staticmethod
    def adapt_input(data, target_format):
        """é€‚é…è¾“å…¥æ•°æ®åˆ°ç›®æ ‡æ ¼å¼"""
        raise NotImplementedError


class TimeSeriesAdapter(InputAdapter):
    """æ—¶é—´åºåˆ—æ•°æ®é€‚é…å™¨ - å°†1Dæ—¶é—´åºåˆ—è½¬æ¢ä¸º2Dç‰¹å¾å›¾"""

    def __init__(self, input_dim, output_channels=64, feature_size=32):
        """
        Args:
            input_dim: è¾“å…¥æ—¶é—´åºåˆ—çš„ç»´åº¦
            output_channels: è¾“å‡ºç‰¹å¾é€šé“æ•°
            feature_size: è¾“å‡ºç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸
        """
        self.input_dim = input_dim
        self.output_channels = output_channels
        self.feature_size = feature_size

        # åˆ›å»ºæ˜ å°„ç½‘ç»œ
        self.mapper = nn.Sequential(
            nn.Linear(input_dim, output_channels * feature_size * feature_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def adapt_input(self, time_series, timestamps=None):
        """
        å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºODEè¾“å…¥æ ¼å¼

        Args:
            time_series: [B, T, input_dim] æ—¶é—´åºåˆ—æ•°æ®
            timestamps: [T] æ—¶é—´æˆ³ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            dict: åŒ…å«é€‚é…åçš„ODEè¾“å…¥
        """
        batch_size, seq_len, input_dim = time_series.shape

        # 1. æ˜ å°„åˆ°2Dç‰¹å¾å›¾
        flat_series = time_series.view(-1, input_dim)
        features_flat = self.mapper(flat_series)
        features_2d = features_flat.view(
            batch_size, seq_len, self.output_channels,
            self.feature_size, self.feature_size
        )

        # 2. å‡†å¤‡ODEè¾“å…¥æ ¼å¼
        current_input = features_2d[:, -1:, :, :, :]  # æœ€åä¸€å¸§ä½œä¸ºå½“å‰è¾“å…¥
        observations = features_2d  # æ‰€æœ‰å¸§ä½œä¸ºè§‚æµ‹

        # 3. ç”Ÿæˆæ—¶é—´æˆ³
        if timestamps is None:
            timestamps = torch.linspace(0, seq_len-1, seq_len, dtype=torch.float32)

        return {
            'current_input': current_input,  # [B, 1, C, H, W]
            'observations': observations,     # [B, T, C, H, W]
            'times': timestamps              # [T]
        }


class ImageSequenceAdapter(InputAdapter):
    """å›¾åƒåºåˆ—é€‚é…å™¨ - å¤„ç†è§†é¢‘/å›¾åƒåºåˆ—"""

    def __init__(self, target_size=(64, 64), target_channels=64):
        """
        Args:
            target_size: ç›®æ ‡ç©ºé—´å°ºå¯¸
            target_channels: ç›®æ ‡é€šé“æ•°
        """
        self.target_size = target_size
        self.target_channels = target_channels

        # ç‰¹å¾æå–å™¨ (å¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„ç½‘ç»œ)
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, target_channels, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(target_size)
        )

    def adapt_input(self, image_sequence, timestamps=None):
        """
        å°†å›¾åƒåºåˆ—è½¬æ¢ä¸ºODEè¾“å…¥æ ¼å¼

        Args:
            image_sequence: [B, T, C, H, W] å›¾åƒåºåˆ—
            timestamps: [T] æ—¶é—´æˆ³

        Returns:
            dict: é€‚é…åçš„ODEè¾“å…¥
        """
        batch_size, seq_len, channels, height, width = image_sequence.shape

        # 1. ç‰¹å¾æå–
        flat_images = image_sequence.view(-1, channels, height, width)
        features = self.feature_extractor(flat_images)

        # 2. æ¢å¤æ—¶åºç»´åº¦
        feature_c, feature_h, feature_w = features.shape[1:]
        features = features.view(batch_size, seq_len, feature_c, feature_h, feature_w)

        # 3. å‡†å¤‡ODEè¾“å…¥
        current_input = features[:, -1:, :, :, :]
        observations = features

        if timestamps is None:
            timestamps = torch.linspace(0, seq_len-1, seq_len, dtype=torch.float32)

        return {
            'current_input': current_input,
            'observations': observations,
            'times': timestamps
        }


class PointCloudAdapter(InputAdapter):
    """ç‚¹äº‘æ•°æ®é€‚é…å™¨ - å°†3Dç‚¹äº‘è½¬æ¢ä¸º2Dç‰¹å¾å›¾"""

    def __init__(self, grid_size=(64, 64), feature_channels=64,
                 x_range=(-50, 50), y_range=(-50, 50)):
        """
        Args:
            grid_size: BEVç½‘æ ¼å°ºå¯¸
            feature_channels: ç‰¹å¾é€šé“æ•°
            x_range: Xè½´èŒƒå›´
            y_range: Yè½´èŒƒå›´
        """
        self.grid_size = grid_size
        self.feature_channels = feature_channels
        self.x_range = x_range
        self.y_range = y_range

        # ç‚¹äº‘ç‰¹å¾å¤„ç†å™¨
        self.point_processor = nn.Sequential(
            nn.Linear(4, 32),  # å‡è®¾è¾“å…¥æ˜¯ [x, y, z, intensity]
            nn.ReLU(),
            nn.Linear(32, feature_channels),
            nn.ReLU()
        )

    def adapt_input(self, point_clouds, timestamps=None):
        """
        å°†ç‚¹äº‘åºåˆ—è½¬æ¢ä¸ºODEè¾“å…¥æ ¼å¼

        Args:
            point_clouds: List[Tensor] é•¿åº¦ä¸ºTçš„ç‚¹äº‘åˆ—è¡¨ï¼Œæ¯ä¸ªç‚¹äº‘å½¢çŠ¶ä¸º[N, 4]
            timestamps: [T] æ—¶é—´æˆ³

        Returns:
            dict: é€‚é…åçš„ODEè¾“å…¥
        """
        seq_len = len(point_clouds)
        batch_size = 1  # å‡è®¾å•batch

        bev_features = []

        for pc in point_clouds:
            # 1. ç‚¹äº‘ç‰¹å¾æå–
            point_features = self.point_processor(pc)  # [N, feature_channels]

            # 2. è½¬æ¢ä¸ºBEVç½‘æ ¼
            bev_grid = self._points_to_bev_grid(pc[:, :2], point_features)
            bev_features.append(bev_grid)

        # 3. å †å ä¸ºåºåˆ—
        bev_sequence = torch.stack(bev_features, dim=1)  # [1, T, C, H, W]

        current_input = bev_sequence[:, -1:, :, :, :]
        observations = bev_sequence

        if timestamps is None:
            timestamps = torch.linspace(0, seq_len-1, seq_len, dtype=torch.float32)

        return {
            'current_input': current_input,
            'observations': observations,
            'times': timestamps
        }

    def _points_to_bev_grid(self, points_xy, features):
        """å°†ç‚¹äº‘æŠ•å½±åˆ°BEVç½‘æ ¼"""
        h, w = self.grid_size

        # å½’ä¸€åŒ–åæ ‡åˆ°ç½‘æ ¼
        x_norm = (points_xy[:, 0] - self.x_range[0]) / (self.x_range[1] - self.x_range[0])
        y_norm = (points_xy[:, 1] - self.y_range[0]) / (self.y_range[1] - self.y_range[0])

        # è½¬æ¢ä¸ºç½‘æ ¼ç´¢å¼•
        x_idx = (x_norm * (w - 1)).long().clamp(0, w - 1)
        y_idx = (y_norm * (h - 1)).long().clamp(0, h - 1)

        # åˆ›å»ºBEVç½‘æ ¼
        bev_grid = torch.zeros(self.feature_channels, h, w, device=features.device)

        # ç®€å•çš„æœ€å¤§æ± åŒ–èšåˆ
        for i in range(len(points_xy)):
            bev_grid[:, y_idx[i], x_idx[i]] = torch.max(
                bev_grid[:, y_idx[i], x_idx[i]], features[i]
            )

        return bev_grid.unsqueeze(0)  # [1, C, H, W]


class MultiModalAdapter(InputAdapter):
    """å¤šæ¨¡æ€æ•°æ®é€‚é…å™¨"""

    def __init__(self, camera_adapter, lidar_adapter, fusion_channels=128):
        """
        Args:
            camera_adapter: ç›¸æœºæ•°æ®é€‚é…å™¨
            lidar_adapter: æ¿€å…‰é›·è¾¾æ•°æ®é€‚é…å™¨
            fusion_channels: èåˆåçš„ç‰¹å¾é€šé“æ•°
        """
        self.camera_adapter = camera_adapter
        self.lidar_adapter = lidar_adapter

        # ç‰¹å¾èåˆå±‚
        total_channels = (camera_adapter.target_channels +
                         lidar_adapter.feature_channels)
        self.fusion_layer = nn.Sequential(
            nn.Conv2d(total_channels, fusion_channels, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(fusion_channels, fusion_channels, 3, padding=1),
            nn.ReLU()
        )

    def adapt_input(self, camera_data, lidar_data,
                   camera_timestamps=None, lidar_timestamps=None):
        """
        èåˆå¤šæ¨¡æ€æ•°æ®

        Args:
            camera_data: ç›¸æœºæ•°æ®
            lidar_data: æ¿€å…‰é›·è¾¾æ•°æ®
            camera_timestamps: ç›¸æœºæ—¶é—´æˆ³
            lidar_timestamps: æ¿€å…‰é›·è¾¾æ—¶é—´æˆ³

        Returns:
            dict: èåˆåçš„ODEè¾“å…¥
        """
        # 1. åˆ†åˆ«é€‚é…å„æ¨¡æ€æ•°æ®
        camera_adapted = self.camera_adapter.adapt_input(camera_data, camera_timestamps)
        lidar_adapted = self.lidar_adapter.adapt_input(lidar_data, lidar_timestamps)

        # 2. æ—¶é—´å¯¹é½ (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ä¸­éœ€è¦æ›´å¤æ‚çš„å¯¹é½)
        # è¿™é‡Œå‡è®¾ä¸¤ä¸ªæ¨¡æ€çš„æœ€åæ—¶åˆ»å¯¹é½
        camera_current = camera_adapted['current_input']  # [B, 1, C1, H, W]
        lidar_current = lidar_adapted['current_input']    # [B, 1, C2, H, W]

        # 3. ç‰¹å¾èåˆ
        fused_current = torch.cat([camera_current, lidar_current], dim=2)  # [B, 1, C1+C2, H, W]
        fused_current = self.fusion_layer(fused_current.squeeze(1)).unsqueeze(1)

        # 4. ä¸ºè§‚æµ‹åºåˆ—ä¹Ÿè¿›è¡Œèåˆ (ç®€åŒ–å¤„ç†)
        fused_obs = fused_current.repeat(1, max(len(camera_timestamps or [0]),
                                               len(lidar_timestamps or [0])), 1, 1, 1)

        return {
            'current_input': fused_current,
            'observations': fused_obs,
            'times': camera_timestamps or lidar_timestamps or torch.tensor([0.0]),
            'camera_data': camera_adapted,
            'lidar_data': lidar_adapted
        }


# ä¾¿æ·çš„é€‚é…å™¨å·¥å‚å‡½æ•°
def create_adapter(data_type, **kwargs):
    """
    æ ¹æ®æ•°æ®ç±»å‹åˆ›å»ºé€‚é…å™¨

    Args:
        data_type: 'timeseries', 'images', 'pointcloud', 'multimodal'
        **kwargs: é€‚é…å™¨å‚æ•°

    Returns:
        é€‚é…å™¨å®ä¾‹
    """
    adapters = {
        'timeseries': TimeSeriesAdapter,
        'images': ImageSequenceAdapter,
        'pointcloud': PointCloudAdapter,
        'multimodal': MultiModalAdapter
    }

    if data_type not in adapters:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")

    return adapters[data_type](**kwargs)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ”§ ODEæ¨¡å—è¾“å…¥é€‚é…å™¨ç¤ºä¾‹")

    # æ—¶é—´åºåˆ—ç¤ºä¾‹
    ts_adapter = TimeSeriesAdapter(input_dim=10, output_channels=64)
    time_series = torch.randn(2, 5, 10)  # [batch, time, features]
    ts_adapted = ts_adapter.adapt_input(time_series)
    print(f"æ—¶é—´åºåˆ—é€‚é…: {ts_adapted['current_input'].shape}")

    # å›¾åƒåºåˆ—ç¤ºä¾‹
    img_adapter = ImageSequenceAdapter(target_channels=64)
    images = torch.randn(1, 4, 3, 128, 128)  # [batch, time, channels, h, w]
    img_adapted = img_adapter.adapt_input(images)
    print(f"å›¾åƒåºåˆ—é€‚é…: {img_adapted['current_input'].shape}")

    print("âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸ!")